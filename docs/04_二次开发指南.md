# 二次开发指南

> **摘要**：本文档为开发者提供二次开发的完整指南，包括算法扩展、环境修改、奖励函数调整的具体方法，以及建议创建的子智能体团队规划。

---

## 一、算法扩展指南

### 1.1 需要修改的文件

| 修改目标 | 文件路径 | 修改内容 |
|---------|---------|---------|
| 新算法实现 | `onpolicy/algorithms/` | 创建新算法目录或文件 |
| 训练集成 | `train_ppo.py` | 集成新算法，修改训练循环 |
| 新Buffer | `onpolicy/utils/` | 如果需要不同的经验存储方式 |

### 1.2 策略接口约定

**必须实现的接口**：

```python
class NewPolicy(nn.Module):
    def __init__(self, args, obs_space, action_space, device):
        """初始化网络"""
        pass
    
    def get_actions(self, obs, rnn_states_actor, rnn_states_critic, 
                    masks, available_actions=None, deterministic=False):
        """
        采样动作（推理用）
        
        Args:
            obs: 观测 (batch_size, obs_dim)
            rnn_states_actor: Actor RNN状态
            rnn_states_critic: Critic RNN状态
            masks: 掩码 (batch_size, 1)
            deterministic: 是否确定性采样
        
        Returns:
            values: 价值估计 (batch_size, 1)
            actions: 动作 (batch_size, action_dim)
            action_log_probs: log概率 (batch_size, 1)
            rnn_states_actor: 更新后的Actor RNN状态
            rnn_states_critic: 更新后的Critic RNN状态
        """
        pass
    
    def get_values(self, obs, rnn_states_critic, masks):
        """
        获取价值估计
        
        Returns:
            values: 价值估计 (batch_size, 1)
        """
        pass
    
    def evaluate_actions(self, obs, rnn_states_actor, rnn_states_critic,
                        action, masks, available_actions=None, active_masks=None):
        """
        评估动作（训练用）
        
        Returns:
            values: 价值估计
            action_log_probs: log概率
            dist_entropy: 分布熵
        """
        pass
```

### 1.3 示例：实现DQN算法

**Step 1: 创建DQN网络**

```python
# onpolicy/algorithms/dqn_network.py

import torch
import torch.nn as nn
from onpolicy.algorithms.utils.mlp import MLPBase

class DQNNetwork(nn.Module):
    def __init__(self, args, obs_space, action_space, device):
        super().__init__()
        self.device = device
        self.action_dim = action_space.shape[0]
        
        # Q网络
        self.q_net = MLPBase(args, obs_space.shape)
        self.q_out = nn.Linear(args.hidden_size, self.action_dim)
        
        # 目标网络
        self.target_q_net = MLPBase(args, obs_space.shape)
        self.target_q_out = nn.Linear(args.hidden_size, self.action_dim)
        
        self.update_target()
    
    def update_target(self):
        """更新目标网络"""
        self.target_q_net.load_state_dict(self.q_net.state_dict())
        self.target_q_out.load_state_dict(self.q_out.state_dict())
    
    def forward(self, obs):
        """获取Q值"""
        features = self.q_net(obs)
        q_values = self.q_out(features)
        return q_values
```

**Step 2: 创建DQN Buffer**

```python
# onpolicy/utils/dqn_buffer.py

import numpy as np
import random

class DQNReplayBuffer:
    def __init__(self, capacity=10000):
        self.capacity = capacity
        self.buffer = []
    
    def push(self, obs, action, reward, next_obs, done):
        """存储经验"""
        if len(self.buffer) >= self.capacity:
            self.buffer.pop(0)
        self.buffer.append((obs, action, reward, next_obs, done))
    
    def sample(self, batch_size):
        """采样batch"""
        batch = random.sample(self.buffer, batch_size)
        obs, actions, rewards, next_obs, dones = zip(*batch)
        return (
            np.array(obs),
            np.array(actions),
            np.array(rewards),
            np.array(next_obs),
            np.array(dones)
        )
```

**Step 3: 创建DQN训练脚本**

```python
# train_dqn.py

class DQNTrainer:
    def __init__(self, args):
        self.env = SheepFlockEnv(...)
        self.policy = DQNNetwork(args, ...)
        self.buffer = DQNReplayBuffer()
        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=args.lr)
    
    def train_step(self, batch_size=32):
        obs, actions, rewards, next_obs, dones = self.buffer.sample(batch_size)
        
        # 计算当前Q值
        q_values = self.policy(obs).gather(1, actions)
        
        # 计算目标Q值
        with torch.no_grad():
            next_q_values = self.policy.target_q_net(next_obs).max(1)
            target_q = rewards + gamma * next_q_values * (1 - dones)
        
        # 计算损失
        loss = nn.MSELoss()(q_values, target_q)
        
        # 更新
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
```

---

## 二、环境扩展指南

### 2.1 需要修改的文件

| 修改目标 | 文件路径 | 修改内容 |
|---------|---------|---------|
| 羊群行为 | `envs/sheep_entity.py` | 修改Boids规则、添加新行为 |
| 场景元素 | `envs/sheep_scenario.py` | 添加障碍物、新目标类型 |
| 环境接口 | `envs/sheep_flock.py` | 修改观测/动作空间、奖励函数 |

### 2.2 环境接口约定

**必须遵循Gym规范**：

```python
class NewEnv:
    observation_space: gym.spaces.Box    # 观测空间
    action_space: gym.spaces.Box         # 动作空间
    
    def reset() -> np.ndarray:
        """重置环境，返回初始观测"""
        pass
    
    def step(actions) -> Tuple[obs, reward, done, info]:
        """执行动作，返回(观测, 奖励, 结束标志, 信息)"""
        pass
    
    def render(mode='human'):
        """渲染环境"""
        pass
    
    def close():
        """关闭环境"""
        pass
```

### 2.3 示例：添加障碍物

**Step 1: 修改 SheepScenario**

```python
# envs/sheep_scenario.py

class SheepScenario:
    def __init__(self, ...):
        # 新增障碍物属性
        self.obstacles = []
        self.num_obstacles = 3
    
    def _init_obstacles(self):
        """初始化障碍物"""
        self.obstacles = []
        for _ in range(self.num_obstacles):
            pos = np.random.uniform(
                [self.world_size[0] * 0.2, self.world_size[1] * 0.2],
                [self.world_size[0] * 0.8, self.world_size[1] * 0.8]
            )
            radius = np.random.uniform(1.5, 3.0)
            self.obstacles.append({
                'position': pos,
                'radius': radius
            })
    
    def check_obstacle_collision(self, position):
        """检查位置是否与障碍物碰撞"""
        for obs in self.obstacles:
            distance = np.linalg.norm(position - obs['position'])
            if distance < obs['radius']:
                return True
        return False
    
    def reset(self, ...):
        self._init_sheep()
        self._init_herders()
        self._init_obstacles()  # 新增
```

**Step 2: 修改 SheepEntity 添加障碍物规避**

```python
# envs/sheep_entity.py

class SheepEntity:
    def obstacle_avoidance(self, obstacles, avoidance_radius=5.0):
        """障碍物规避行为"""
        steer = np.zeros(2, dtype=np.float32)
        count = 0
        
        for obs in obstacles:
            diff = self.position - obs['position']
            distance = np.linalg.norm(diff) - obs['radius']
            
            if 0 < distance < avoidance_radius:
                # 按距离反比加权
                steer += diff / (distance ** 2 + 1e-6)
                count += 1
        
        if count > 0:
            steer = steer / count
            if np.linalg.norm(steer) > 0:
                steer = steer / np.linalg.norm(steer) * self.max_speed * 1.5
                steer = steer - self.velocity
                if np.linalg.norm(steer) > self.max_force * 2:
                    steer = steer / np.linalg.norm(steer) * self.max_force * 2
        
        return steer
    
    def apply_boids_rules(self, all_sheep, herders, world_size, obstacles=None, weights=None):
        # 原有规则...
        
        # 新增障碍物规避
        if obstacles:
            obs_force = self.obstacle_avoidance(obstacles) * weights.get('obstacle', 1.5)
            self.apply_force(obs_force)
```

**Step 3: 修改 SheepFlockEnv 扩展观测空间**

```python
# envs/sheep_flock.py

class SheepFlockEnv:
    def _setup_spaces(self):
        # 扩展观测维度：添加障碍物信息
        # 假设最多3个障碍物，每个2维位置
        self.obs_dim = 10 + 3 * 2  # 原有10 + 6
        
        self.observation_space = spaces.Box(
            low=-np.inf,
            high=np.inf,
            shape=(self.obs_dim,),
            dtype=np.float32,
        )
    
    def _get_obs(self):
        obs = self.scenario.get_observation()
        
        # 添加障碍物位置
        obstacle_obs = np.zeros(6, dtype=np.float32)  # 3个障碍物 × 2维
        for i, obs_info in enumerate(self.scenario.obstacles[:3]):
            obstacle_obs[i*2:(i+1)*2] = obs_info['position'] / self.world_size[0]
        
        return np.concatenate([obs, obstacle_obs])
    
    def _compute_reward(self):
        reward = super()._compute_reward()
        
        # 新增：碰撞惩罚
        for sheep in self.scenario.sheep:
            if self.scenario.check_obstacle_collision(sheep.position):
                reward -= self.reward_config.get('obstacle_collision_penalty', 0.5)
        
        return reward
```

---

## 三、奖励函数调整指南

### 3.1 需要修改的文件

| 文件路径 | 修改内容 |
|---------|---------|
| `envs/sheep_flock.py` | `_compute_reward()` 方法 |
| `train_ppo.py` | 奖励配置参数 |

### 3.2 当前奖励函数结构

```python
def _compute_reward(self):
    reward = 0.0
    
    # 1. 距离奖励：羊群向目标移动
    distance_delta = self.prev_distance - current_distance
    reward += distance_delta * distance_reward_weight
    
    # 2. 扩散惩罚：羊群过于分散
    if spread > 10.0:
        reward -= (spread - 10.0) * spread_penalty_weight
    
    # 3. 成功奖励：羊群到达目标
    if is_flock_at_target:
        reward += success_bonus
    
    return reward
```

### 3.3 示例：添加复杂奖励

```python
def _compute_reward(self):
    reward = 0.0
    
    # ===== 原有奖励 =====
    
    # 1. 距离奖励
    current_distance = self.scenario.get_distance_to_target()
    if self.prev_distance is not None:
        distance_delta = self.prev_distance - current_distance
        reward += distance_delta * self.reward_config['distance_reward_weight']
    self.prev_distance = current_distance
    
    # 2. 扩散惩罚
    spread = self.scenario.get_flock_spread()
    if spread > 10.0:
        reward -= (spread - 10.0) * self.reward_config['spread_penalty_weight']
    
    # 3. 成功奖励
    if self.scenario.is_flock_at_target(threshold=5.0):
        reward += self.reward_config['success_bonus']
    
    # ===== 新增奖励 =====
    
    # 4. 速度对齐奖励：鼓励羊群向目标方向移动
    avg_velocity = np.mean([s.velocity for s in self.scenario.sheep], axis=0)
    avg_speed = np.linalg.norm(avg_velocity)
    if avg_speed > 1e-6:
        target_direction = self.scenario.target_position - self.scenario.get_flock_center()
        target_direction = target_direction / (np.linalg.norm(target_direction) + 1e-6)
        alignment = np.dot(avg_velocity / avg_speed, target_direction)
        reward += alignment * self.reward_config.get('velocity_alignment_weight', 0.5)
    
    # 5. 站位奖励：鼓励合理站位
    herder_positions = self.scenario.get_herder_positions()
    flock_center = self.scenario.get_flock_center()
    avg_herder_distance = np.mean([np.linalg.norm(h - flock_center) for h in herder_positions])
    optimal_distance = 8.0  # 理想站位距离
    distance_penalty = abs(avg_herder_distance - optimal_distance) * 0.1
    reward -= distance_penalty
    
    # 6. 包围奖励：鼓励机械狗均匀分布
    if len(herder_positions) > 1:
        angles = []
        for h in herder_positions:
            vec = h - flock_center
            angles.append(np.arctan2(vec[1], vec[0]))
        angles = np.sort(angles)
        angle_diffs = np.diff(np.concatenate([angles, [angles[0] + 2*np.pi]]))
        angle_uniformity = np.std(angle_diffs)  # 角度差异的标准差
        reward -= angle_uniformity * self.reward_config.get('formation_weight', 0.2)
    
    # 7. 时间惩罚：鼓励快速完成任务
    reward -= self.reward_config.get('time_penalty_weight', 0.01)
    
    # 8. 边界惩罚：避免羊群靠近边界
    for sheep in self.scenario.sheep:
        margin = 5.0
        if (sheep.position[0] < margin or sheep.position[0] > self.world_size[0] - margin or
            sheep.position[1] < margin or sheep.position[1] > self.world_size[1] - margin):
            reward -= self.reward_config.get('boundary_penalty', 0.1)
    
    return float(reward)
```

---

## 四、子智能体团队规划

### 4.1 智能体架构

```
┌─────────────────────────────────────────────────────────────┐
│                    主工程师智能体 (Main Agent)                │
│                    职责：任务规划、协调调度、决策确认           │
└───────────────────────────┬─────────────────────────────────┘
                            │
        ┌───────────────────┼───────────────────┐
        │                   │                   │
        ▼                   ▼                   ▼
┌───────────────┐   ┌───────────────┐   ┌───────────────┐
│ 环境优化智能体  │   │ 算法实验智能体  │   │ 可视化评估智能体│
│ (env-agent)   │   │ (algo-agent)  │   │ (viz-agent)   │
└───────────────┘   └───────────────┘   └───────────────┘
```

### 4.2 各智能体详细说明

#### 4.2.1 环境优化智能体 (env-agent)

**英文标识名**: `env-agent`

**职责**：
- 修改羊群行为模型（Boids规则调整）
- 添加新的场景元素（障碍物、目标类型）
- 调整观测空间和动作空间
- 优化奖励函数设计

**可被调用的时机**：
- 需要修改羊群行为时
- 需要添加障碍物时
- 需要调整奖励函数时
- 需要修改环境参数时
- 需要调试环境逻辑时

**核心能力**：
- 修改 `envs/sheep_entity.py` 中的Boids规则
- 修改 `envs/sheep_scenario.py` 中的场景管理
- 修改 `envs/sheep_flock.py` 中的环境接口
- 编写环境单元测试

**示例任务**：
```
任务：添加圆形障碍物
1. 在SheepScenario中添加obstacles属性
2. 实现_init_obstacles方法
3. 在SheepEntity中添加obstacle_avoidance行为
4. 修改观测空间包含障碍物信息
5. 添加碰撞惩罚到奖励函数
6. 编写测试验证功能
```

#### 4.2.2 算法实验智能体 (algo-agent)

**英文标识名**: `algo-agent`

**职责**：
- 实现新的RL算法（DQN, SAC, TD3等）
- 调整现有算法超参数
- 实现新的网络结构
- 进行算法对比实验

**可被调用的时机**：
- 需要尝试新算法时
- 需要调整超参数时
- 需要修改网络结构时
- 需要进行消融实验时
- 需要对比不同算法效果时

**核心能力**：
- 在 `onpolicy/algorithms/` 下创建新算法
- 修改 `train_ppo.py` 集成新算法
- 实现新的网络层和激活函数
- 编写算法单元测试

**示例任务**：
```
任务：实现SAC算法
1. 创建onpolicy/algorithms/sac/目录
2. 实现SAC网络结构（Actor, Critic1, Critic2）
3. 实现SAC训练器（含自动温度调整）
4. 创建SAC专用Buffer
5. 修改train_ppo.py添加SAC选项
6. 编写测试验证功能
7. 对比PPO和SAC的训练效果
```

#### 4.2.3 可视化评估智能体 (viz-agent)

**英文标识名**: `viz-agent`

**职责**：
- 实现训练过程可视化
- 实现环境渲染
- 生成评估报告
- 创建演示视频

**可被调用的时机**：
- 需要可视化训练过程时
- 需要生成评估报告时
- 需要创建演示材料时
- 需要调试环境行为时
- 需要对比不同实验结果时

**核心能力**：
- 使用TensorBoard/Wandb记录训练指标
- 使用Matplotlib绘制学习曲线
- 实现环境渲染（2D/3D）
- 生成评估报告和演示视频

**示例任务**：
```
任务：创建训练可视化面板
1. 实现TensorBoard日志记录
2. 绘制奖励曲线、损失曲线
3. 记录羊群状态变化动画
4. 生成评估报告
5. 创建对比图表
```

### 4.3 智能体协作模式

#### 模式1：串行协作
```
主智能体 → 环境智能体（修改环境）→ 算法智能体（适配算法）→ 可视化智能体（评估效果）
```

#### 模式2：并行协作
```
主智能体 → 同时调用：
           ├── 环境智能体（独立修改环境）
           ├── 算法智能体（独立实验算法）
           └── 可视化智能体（独立生成报告）
```

#### 模式3：迭代协作
```
主智能体 → 环境智能体 → 算法智能体 → 可视化智能体 → 主智能体（评估结果）
                                    ↑
                                    └── 反馈循环
```

### 4.4 智能体调用示例

```python
# 主智能体调用示例

# 任务：添加障碍物并评估效果
def add_obstacles_and_evaluate():
    # 1. 调用环境智能体添加障碍物
    env_agent.add_circular_obstacles(num_obstacles=3)
    
    # 2. 调用算法智能体重新训练
    algo_agent.train_with_current_env(episodes=1000)
    
    # 3. 调用可视化智能体生成报告
    viz_agent.generate_comparison_report(
        baseline="no_obstacles",
        current="with_obstacles"
    )

# 任务：对比不同算法
def compare_algorithms():
    algorithms = ["ppo", "dqn", "sac"]
    
    for algo in algorithms:
        # 调用算法智能体训练
        algo_agent.train(algorithm=algo, episodes=2000)
        
    # 调用可视化智能体生成对比报告
    viz_agent.generate_algorithm_comparison(algorithms)
```

---

## 五、开发工作流建议

### 5.1 新功能开发流程

1. **需求分析**：主智能体分析需求，确定涉及的模块
2. **任务分解**：将需求分解为子任务
3. **智能体分配**：根据任务类型分配给合适的子智能体
4. **并行执行**：独立任务并行执行
5. **集成测试**：主智能体协调集成和测试
6. **验收确认**：用户确认结果

### 5.2 代码质量保障

- 每个子智能体完成任务后需运行相关测试
- 主智能体负责代码审查
- 重要修改需用户确认

### 5.3 文档更新

- 新功能需更新相关文档
- API变更需更新接口文档
- 实验结果需记录到实验日志

---

## 六、快速参考

### 6.1 关键文件速查

| 功能 | 文件 |
|------|------|
| 主训练入口 | `train_ppo.py` |
| 环境定义 | `envs/sheep_flock.py` |
| 羊群行为 | `envs/sheep_entity.py` |
| 场景管理 | `envs/sheep_scenario.py` |
| Actor-Critic | `onpolicy/algorithms/ppo_actor_critic.py` |
| 有界动作 | `onpolicy/algorithms/utils/bounded_act.py` |
| 经验Buffer | `onpolicy/utils/ppo_buffer.py` |

### 6.2 常用命令

```bash
# 训练
python train_ppo.py --num_env_steps 1000000 --episode_length 100

# 测试
python -m unittest tests.test_ppo -v
python tests/test_env.py

# 清理缓存
find . -type d -name "__pycache__" -exec rm -rf {} +
```

### 6.3 配置参数速查

| 参数 | 默认值 | 说明 |
|------|--------|------|
| `--lr` | 5e-4 | 学习率 |
| `--ppo_epoch` | 15 | PPO更新轮数 |
| `--clip_param` | 0.2 | PPO裁剪参数 |
| `--num_sheep` | 10 | 羊的数量 |
| `--num_herders` | 3 | 机械狗数量 |
| `--episode_length` | 100 | Episode长度 |
| `--hidden_size` | 64 | 隐藏层大小 |
