# 牧羊场景 RL 环境设计文档

> **摘要**：本文档详细描述了牧羊场景的物理规则、状态空间、动作空间、奖励函数设计，以及每个时间步的更新逻辑，为环境扩展和二次开发提供参考。

---

## 一、场景概述

### 1.1 场景描述

本项目模拟一个**羊群引导任务**：
- **目标**：控制多个机械狗（Herders）将羊群（Sheep）引导到指定目标位置
- **特点**：采用**高层决策控制器**，输出站位参数而非直接位置控制
- **挑战**：羊群具有群体智能行为（Boids模型），需要策略性地围堵和引导

### 1.2 场景元素

| 元素 | 数量 | 说明 |
|------|------|------|
| 羊群 (Sheep) | 默认10只 | 具有Boids群体行为 |
| 机械狗 (Herders) | 默认3只 | 执行站位策略 |
| 目标位置 (Target) | 1个 | 羊群需要到达的位置 |
| 世界边界 | 矩形 | 默认50×50单位 |

---

## 二、物理规则

### 2.1 羊群运动模型（Boids模型）

每只羊遵循 **Boids** 群体行为规则，包含5种行为力：

```
总受力 = 分离力 × w_sep + 对齐力 × w_ali + 聚合力 × w_coh + 逃避力 × w_eva + 边界力 × w_bnd
```

#### 2.1.1 分离规则 (Separation)

**目的**：避免与邻近羊碰撞

```
分离力计算流程：
1. 遍历感知范围内的邻居
2. 计算与每个邻居的方向向量 diff = self.position - neighbor.position
3. 按距离反比加权：diff / (distance² + ε)
4. 求平均方向
5. 归一化到最大速度
6. 减去当前速度得到转向力
7. 限制到最大转向力
```

**参数**：
- `separation_radius`: 分离感知半径，默认2.0
- `weight`: 默认1.5

#### 2.1.2 对齐规则 (Alignment)

**目的**：与邻近羊保持相同运动方向

```
对齐力计算流程：
1. 遍历感知范围内的邻居
2. 累加邻居速度向量
3. 计算平均速度
4. 归一化到最大速度
5. 减去当前速度得到转向力
6. 限制到最大转向力
```

**参数**：
- `perception_radius`: 感知半径，默认5.0
- `weight`: 默认1.0

#### 2.1.3 聚合规则 (Cohesion)

**目的**：向邻近羊群的中心移动

```
聚合力计算流程：
1. 遍历感知范围内的邻居
2. 计算邻居位置的中心点
3. 向中心点施加seek力
```

**参数**：
- `perception_radius`: 感知半径，默认5.0
- `weight`: 默认1.0

#### 2.1.4 逃避规则 (Evasion)

**目的**：远离机械狗

```
逃避力计算流程：
1. 遍历所有机械狗位置
2. 计算与机械狗的方向向量
3. 如果距离 < 逃避半径，按距离反比加权
4. 求平均逃避方向
5. 速度提升到1.5倍最大速度
6. 转向力提升到2倍最大力
```

**参数**：
- `evasion_radius`: 逃避感知半径，默认8.0
- `weight`: 默认2.0（最高优先级）

#### 2.1.5 边界力 (Boundary)

**目的**：避免羊跑出世界边界

```
边界力计算流程：
1. 如果 x < margin: 施加正向x力
2. 如果 x > world_width - margin: 施加负向x力
3. y方向同理
```

**参数**：
- `margin`: 边界安全距离，默认2.0
- `weight`: 默认1.0

### 2.2 羊的运动更新

```python
def update(dt):
    # 1. 速度更新
    velocity += acceleration × dt
    
    # 2. 速度限制
    if speed > max_speed:
        velocity = velocity / speed × max_speed
    
    # 3. 位置更新
    position += velocity × dt
    
    # 4. 清空加速度
    acceleration = 0
```

**参数**：
- `max_speed`: 最大速度，默认1.0
- `max_force`: 最大转向力，默认0.1
- `dt`: 时间步长，默认0.1

### 2.3 机械狗站位采样

机械狗不直接控制位置，而是通过**站位参数**采样：

```
站位采样流程：
1. 获取羊群质心位置 flock_center
2. 对每个机械狗 i：
   a. 计算基础角度偏移: angle_offset = 2π × i / num_herders
   b. 计算实际角度: angle = angle_mean + angle_offset + random_noise
   c. 采样半径: radius = normal(radius_mean, radius_std × (1 - concentration))
   d. 计算位置: position = flock_center + [radius × cos(angle), radius × sin(angle)]
   e. 裁剪到世界边界
```

---

## 三、状态空间设计

### 3.1 观测空间 (Observation Space)

**形状**: `(10,)` - 10维向量

| 索引 | 内容 | 归一化 | 说明 |
|------|------|--------|------|
| [0:2] | 羊群质心位置 | ÷ world_size | 归一化到[0,1] |
| [2:4] | 羊群主方向向量 | - | 单位向量 |
| [4] | 羊群扩散度 | ÷ 10.0 | 归一化 |
| [5] | 羊群数量 | ÷ 30.0 | 归一化 |
| [6:8] | 目标位置 | ÷ world_size | 归一化到[0,1] |
| [8:10] | 第一个机械狗位置 | ÷ world_size | 归一化到[0,1] |

**代码实现**：
```python
def get_observation(self):
    obs = np.zeros(10, dtype=np.float32)
    obs[0:2] = flock_center / world_size
    obs[2:4] = flock_direction  # 单位向量
    obs[4] = flock_spread / 10.0
    obs[5] = num_sheep / 30.0
    obs[6:8] = target_position / world_size
    obs[8:10] = herder_positions[0] / world_size
    return obs
```

### 3.2 动作空间 (Action Space)

**形状**: `(4,)` - 4维连续向量

| 索引 | 参数名 | 范围 | 说明 |
|------|--------|------|------|
| [0] | radius_mean | [0, 20] | 站位半径均值 |
| [1] | radius_std | [0, 5] | 站位半径标准差 |
| [2] | angle_mean | [-π, π] | 聚集角度均值 |
| [3] | concentration | [0, 1] | 聚集度（越高站位越集中） |

**动作语义**：
- `radius_mean`: 机械狗围绕羊群质心的站位距离
- `radius_std`: 站位半径的变化范围
- `angle_mean`: 机械狗围绕羊群质心的角度基准
- `concentration`: 站位的集中程度，1表示精确站位，0表示随机分散

---

## 四、奖励函数设计

### 4.1 当前奖励函数

```python
def _compute_reward(self):
    reward = 0.0
    
    # 1. 距离奖励：鼓励羊群向目标移动
    distance_delta = prev_distance - current_distance
    reward += distance_delta × distance_reward_weight  # 默认1.0
    
    # 2. 扩散惩罚：避免羊群过于分散
    if spread > 10.0:
        reward -= (spread - 10.0) × spread_penalty_weight  # 默认0.1
    
    # 3. 成功奖励：羊群到达目标
    if distance_to_target < 5.0:
        reward += success_bonus  # 默认10.0
    
    return reward
```

### 4.2 奖励配置参数

| 参数 | 默认值 | 说明 |
|------|--------|------|
| `distance_reward_weight` | 1.0 | 距离奖励权重 |
| `spread_penalty_weight` | 0.1 | 扩散惩罚权重 |
| `success_bonus` | 10.0 | 成功奖励 |
| `timeout_penalty` | -5.0 | 超时惩罚（未实现） |
| `collision_penalty` | -0.5 | 碰撞惩罚（未实现） |

---

## 五、时间步更新流程

### 5.1 完整时间步流程图

```
┌─────────────────────────────────────────────────────────────────┐
│                    一个时间步的完整流程                           │
└─────────────────────────────────────────────────────────────────┘

输入: actions (站位参数)
      │
      ▼
┌─────────────────────────────────────────┐
│ 1. 站位采样                              │
│    _sample_herder_positions(actions)    │
│    - 根据参数采样机械狗位置              │
│    - 围绕羊群质心分布                    │
└────────────────────┬────────────────────┘
                     │
                     ▼
┌─────────────────────────────────────────┐
│ 2. 更新机械狗位置                        │
│    scenario.update_herders(positions)   │
│    - 裁剪到世界边界                      │
└────────────────────┬────────────────────┘
                     │
                     ▼
┌─────────────────────────────────────────┐
│ 3. 更新羊群状态                          │
│    scenario.update_sheep(dt)            │
│    ┌─────────────────────────────────┐  │
│    │ 对每只羊:                        │  │
│    │   a. 获取邻居列表                │  │
│    │   b. 计算分离力                  │  │
│    │   c. 计算对齐力                  │  │
│    │   d. 计算聚合力                  │  │
│    │   e. 计算逃避力（远离机械狗）    │  │
│    │   f. 计算边界力                  │  │
│    │   g. 合并所有力                  │  │
│    │   h. 更新速度和位置              │  │
│    └─────────────────────────────────┘  │
└────────────────────┬────────────────────┘
                     │
                     ▼
┌─────────────────────────────────────────┐
│ 4. 计算奖励                              │
│    _compute_reward()                    │
│    - 距离变化奖励                        │
│    - 扩散惩罚                            │
│    - 成功奖励                            │
└────────────────────┬────────────────────┘
                     │
                     ▼
┌─────────────────────────────────────────┐
│ 5. 检查终止条件                          │
│    _check_done()                        │
│    - 羊群到达目标？                      │
│    - 达到最大步数？                      │
└────────────────────┬────────────────────┘
                     │
                     ▼
┌─────────────────────────────────────────┐
│ 6. 获取新观测                            │
│    _get_obs()                           │
│    - 计算羊群质心、方向、扩散度          │
│    - 组装观测向量                        │
└────────────────────┬────────────────────┘
                     │
                     ▼
输出: (obs, reward, done, info)
```

### 5.2 代码实现

```python
def step(self, actions):
    self.current_step += 1
    
    # Step 1 & 2: 站位采样并更新机械狗位置
    herder_positions = self._sample_herder_positions(actions)
    self.scenario.update_herders(herder_positions)
    
    # Step 3: 更新羊群状态
    self.scenario.update_sheep(self.dt)
    
    # Step 4: 计算奖励
    reward = self._compute_reward()
    
    # Step 5: 检查终止
    done = self._check_done()
    
    # Step 6: 获取观测
    obs = self._get_obs()
    info = self._get_info()
    
    return obs, reward, done, info
```

---

## 六、二次开发关键点

### 6.1 新增实体类型

**修改文件**: `envs/sheep_entity.py`, `envs/sheep_scenario.py`

**示例：添加障碍物**

```python
# 1. 在 SheepScenario 中添加障碍物属性
class SheepScenario:
    def __init__(self, ...):
        self.obstacles = []  # 新增
    
    def _init_obstacles(self, num_obstacles=3):
        for _ in range(num_obstacles):
            pos = np.random.uniform([5, 5], [world_size[0]-5, world_size[1]-5])
            radius = np.random.uniform(1, 3)
            self.obstacles.append({'position': pos, 'radius': radius})

# 2. 在 SheepEntity 中添加障碍物规避行为
class SheepEntity:
    def obstacle_avoidance(self, obstacles):
        steer = np.zeros(2)
        for obs in obstacles:
            diff = self.position - obs['position']
            distance = np.linalg.norm(diff) - obs['radius']
            if distance < self.perception_radius:
                steer += diff / (distance ** 2 + 1e-6)
        return steer
```

### 6.2 修改羊群行为

**修改文件**: `envs/sheep_entity.py`

**可调整参数**:
- `max_speed`: 最大移动速度
- `max_force`: 最大转向力
- `perception_radius`: 感知半径
- `separation_radius`: 分离半径

**可调整权重** (在 `sheep_scenario.py`):
```python
self.boids_weights = {
    'separation': 1.5,  # 分离权重
    'alignment': 1.0,   # 对齐权重
    'cohesion': 1.0,    # 聚合权重
    'evasion': 2.0,     # 逃避权重
    'boundary': 1.0,    # 边界权重
}
```

### 6.3 修改奖励函数

**修改文件**: `envs/sheep_flock.py` 的 `_compute_reward()` 方法

**示例：添加复杂奖励**

```python
def _compute_reward(self):
    reward = 0.0
    
    # 原有奖励...
    
    # 【新增】速度对齐奖励：鼓励羊群向目标方向移动
    avg_velocity = np.mean([s.velocity for s in self.scenario.sheep], axis=0)
    target_direction = self.scenario.target_position - self.scenario.get_flock_center()
    target_direction = target_direction / (np.linalg.norm(target_direction) + 1e-6)
    alignment = np.dot(avg_velocity / (np.linalg.norm(avg_velocity) + 1e-6), target_direction)
    reward += alignment * velocity_alignment_weight
    
    # 【新增】时间惩罚：鼓励快速完成任务
    reward -= time_penalty_weight
    
    # 【新增】站位奖励：鼓励合理站位
    avg_herder_distance = np.mean([np.linalg.norm(h - flock_center) for h in herder_positions])
    optimal_distance = 8.0
    reward -= abs(avg_herder_distance - optimal_distance) * 0.1
    
    return reward
```

### 6.4 修改观测/动作空间

**修改文件**: `envs/sheep_flock.py`

**示例：扩展观测空间**

```python
def _setup_spaces(self):
    # 扩展观测维度
    self.obs_dim = 14  # 原来是10
    
    # 添加新观测内容
    # obs[10:12] = 所有机械狗的平均位置
    # obs[12:14] = 羊群速度向量
    
def _get_obs(self):
    obs = np.zeros(self.obs_dim, dtype=np.float32)
    # 原有观测...
    obs[10:12] = np.mean(self.scenario.herder_positions, axis=0) / self.world_size[0]
    obs[12:14] = self.scenario.get_flock_direction()
    return obs
```

