# 项目架构白皮书

> **摘要**：本文档详细描述了「强化学习牧羊项目」的技术架构、目录结构、核心模块和配置参数，为二次开发提供全面的技术参考。

---

## 一、技术栈清单

### 1.1 编程语言与版本

| 语言 | 版本 | 用途 |
|------|------|------|
| Python | 3.8+ | 主要开发语言 |

### 1.2 核心框架与库

| 组件 | 版本 | 用途 |
|------|------|------|
| PyTorch | 1.8.1+ | 深度学习框架，构建神经网络 |
| NumPy | 1.19.4+ | 数值计算，数组操作 |
| Gym | 0.10.5 | 强化学习环境接口标准 |

### 1.3 架构模式

本项目采用 **分层架构** 模式：

```
┌─────────────────────────────────────────┐
│           应用层 (Application)          │  ← train_ppo.py
├─────────────────────────────────────────┤
│           算法层 (Algorithm)            │  ← PPO Actor-Critic
├─────────────────────────────────────────┤
│           数据层 (Buffer)               │  ← 经验回放缓冲区
├─────────────────────────────────────────┤
│           环境层 (Environment)          │  ← Gym环境接口
└─────────────────────────────────────────┘
```

---

## 二、目录结构语义映射

### 2.1 项目根目录结构

```
high_layer/
├── train_ppo.py              # 【入口文件】主训练脚本
│
├── envs/                     # 【环境模块】定义强化学习环境
│   ├── __init__.py          # 模块导出
│   ├── sheep_entity.py      # 羊实体类：Boids模型实现
│   ├── sheep_scenario.py    # 场景管理类：状态管理
│   └── sheep_flock.py       # 环境主类：Gym接口实现
│
├── onpolicy/                 # 【算法核心模块】PPO实现
│   ├── algorithms/          # 算法实现
│   │   ├── ppo_actor_critic.py  # PPO Actor-Critic网络
│   │   └── utils/           # 网络工具模块
│   │       ├── mlp.py           # MLP基础网络
│   │       ├── rnn.py           # RNN层实现
│   │       ├── bounded_act.py   # 有界动作层
│   │       ├── popart.py        # PopArt归一化
│   │       └── util.py          # 工具函数
│   │
│   └── utils/               # 工具模块
│       ├── ppo_buffer.py        # PPO经验缓冲区
│       ├── valuenorm.py         # 值归一化
│       ├── util.py              # 通用工具函数
│       └── __init__.py          # 模块导出
│
├── utils/                    # 【全局工具模块】
│   ├── logger.py            # 日志工具
│   └── utils.py             # 通用工具函数
│
├── tests/                    # 【单元测试】
│   ├── __init__.py
│   ├── test_env.py          # 环境测试
│   └── test_ppo.py          # PPO组件测试
│
├── docs/                     # 【文档目录】
│   └── PROJECT_PLAN.md      # 项目计划文档
│
└── README.md                 # 项目说明
```

### 2.2 入口文件说明

| 入口文件 | 用途 | 启动命令 |
|---------|------|---------|
| `train_ppo.py` | 主训练入口 | `python train_ppo.py --env_name sheep_herding` |
| `tests/test_env.py` | 环境单元测试 | `python tests/test_env.py` |
| `tests/test_ppo.py` | PPO组件测试 | `python -m unittest tests.test_ppo` |

---

## 三、核心模块职责

### 3.1 环境模块 (envs/)

| 文件 | 类名 | 职责 |
|------|------|------|
| `sheep_entity.py` | `SheepEntity` | 单个羊实体，实现Boids行为规则（分离、对齐、聚合、逃避） |
| `sheep_scenario.py` | `SheepScenario` | 场景状态管理，管理羊群、机械狗、目标位置 |
| `sheep_flock.py` | `SheepFlockEnv` | Gym环境接口实现，定义观测/动作空间、奖励函数 |
| `sheep_flock.py` | `SheepFlockEnvWrapper` | 多进程环境包装器 |

### 3.2 算法模块 (onpolicy/algorithms/)

| 文件 | 类名 | 职责 |
|------|------|------|
| `ppo_actor_critic.py` | `PPOActor` | 策略网络，输出站位参数 |
| `ppo_actor_critic.py` | `PPOCritic` | 价值网络，估计状态价值 |
| `ppo_actor_critic.py` | `PPOActorCritic` | 组合的Actor-Critic网络 |
| `utils/mlp.py` | `MLPBase` | MLP基础网络层 |
| `utils/bounded_act.py` | `BoundedACTLayer` | 有界动作输出层（使用Tanh squashing） |

### 3.3 数据模块 (onpolicy/utils/)

| 文件 | 类名 | 职责 |
|------|------|------|
| `ppo_buffer.py` | `PPOReplayBuffer` | PPO经验存储，支持GAE计算 |
| `valuenorm.py` | `ValueNorm` | 值归一化工具 |

---

## 四、核心配置参数

### 4.1 训练参数（train_ppo.py）

| 参数 | 默认值 | 说明 |
|------|--------|------|
| `--lr` | 5e-4 | 学习率 |
| `--ppo_epoch` | 15 | PPO更新轮数 |
| `--num_mini_batch` | 1 | Mini-batch数量 |
| `--clip_param` | 0.2 | PPO裁剪参数 |
| `--value_loss_coef` | 1 | 价值损失系数 |
| `--entropy_coef` | 0.01 | 熵正则化系数 |
| `--gamma` | 0.99 | 折扣因子 |
| `--gae_lambda` | 0.95 | GAE参数 |
| `--max_grad_norm` | 10.0 | 梯度裁剪阈值 |

### 4.2 环境参数（train_ppo.py）

| 参数 | 默认值 | 说明 |
|------|--------|------|
| `--num_sheep` | 10 | 羊的数量 |
| `--num_herders` | 3 | 机械狗数量 |
| `--episode_length` | 100 | Episode长度 |
| `--world_size` | [50.0, 50.0] | 世界大小 |
| `--num_env_steps` | 1000000 | 总训练步数 |

### 4.3 网络参数（train_ppo.py）

| 参数 | 默认值 | 说明 |
|------|--------|------|
| `--hidden_size` | 64 | 隐藏层大小 |
| `--layer_N` | 1 | 网络层数 |
| `--use_ReLU` | True | 使用ReLU激活函数 |
| `--use_orthogonal` | True | 使用正交初始化 |
| `--use_feature_normalization` | True | 使用特征归一化 |

### 4.4 奖励配置（sheep_flock.py）

| 参数 | 默认值 | 说明 |
|------|--------|------|
| `distance_reward_weight` | 1.0 | 距离奖励权重 |
| `spread_penalty_weight` | 0.1 | 扩散惩罚权重 |
| `success_bonus` | 10.0 | 成功奖励 |
| `timeout_penalty` | -5.0 | 超时惩罚 |
| `collision_penalty` | -0.5 | 碰撞惩罚 |

---

## 五、关键接口定义

### 5.1 环境接口

```python
class SheepFlockEnv:
    observation_space: gym.spaces.Box    # 观测空间 (10,)
    action_space: gym.spaces.Box         # 动作空间 (4,)
    
    def reset() -> np.ndarray            # 返回初始观测
    def step(actions) -> Tuple[obs, reward, done, info]
    def get_shared_obs() -> np.ndarray   # 返回共享观测
```

### 5.2 策略接口

```python
class PPOActorCritic:
    def get_actions(obs, rnn_states_actor, rnn_states_critic, masks, 
                    available_actions=None, deterministic=False)
        -> Tuple[values, actions, action_log_probs, rnn_states_actor, rnn_states_critic]
    
    def get_values(obs, rnn_states_critic, masks) -> values
    
    def evaluate_actions(obs, rnn_states_actor, rnn_states_critic, action, masks, ...)
        -> Tuple[values, action_log_probs, dist_entropy]
```

### 5.3 缓冲区接口

```python
class PPOReplayBuffer:
    def insert(obs, rnn_states_actor, rnn_states_critic, actions, ...)
    def compute_returns(next_value, value_normalizer=None)
    def feed_forward_generator(advantages, num_mini_batch) -> Generator
```
