# 算法实现详解文档

> **摘要**：本文档详细描述了项目中使用的 PPO（Proximal Policy Optimization）算法实现，包括网络结构、训练流程、经验回放方式，以及与环境的交互接口。

---

## 一、算法概述

### 1.1 算法选择

本项目使用 **PPO（Proximal Policy Optimization）** 算法，是一种基于策略梯度的强化学习算法。

**选择理由**：
1. **稳定性好**：通过裁剪目标函数，避免策略更新过大
2. **样本效率高**：支持多轮次更新，充分利用收集的经验
3. **易于实现**：相比TRPO等算法，实现简单
4. **适合连续动作**：本项目动作空间为连续值

### 1.2 PPO核心公式

**策略损失（Clipped Surrogate Objective）**：
```
L^CLIP(θ) = E[min(r(θ) × A, clip(r(θ), 1-ε, 1+ε) × A)]

其中：
- r(θ) = π_θ(a|s) / π_θ_old(a|s)  # 重要性采样比率
- A = 优势函数（Advantage）
- ε = 裁剪参数（默认0.2）
```

**价值损失**：
```
L^VF(θ) = E[(V_θ(s) - V^target)²]
```

**熵正则化**：
```
L^ENT(θ) = -E[H(π_θ)]  # 最大化熵，鼓励探索
```

**总损失**：
```
L(θ) = L^CLIP - c1 × L^VF + c2 × L^ENT
```

---

## 二、网络结构

### 2.1 Actor网络（策略网络）

```
输入: observation (10维)
    │
    ▼
┌─────────────────────────────────────┐
│ LayerNorm (特征归一化)              │
└────────────────────┬────────────────┘
                     │
                     ▼
┌─────────────────────────────────────┐
│ Linear(10, 64) + ReLU + LayerNorm   │
└────────────────────┬────────────────┘
                     │
                     ▼
┌─────────────────────────────────────┐
│ Linear(64, 64) + ReLU + LayerNorm   │  × layer_N次
└────────────────────┬────────────────┘
                     │
                     ▼
┌─────────────────────────────────────┐
│ BoundedACTLayer                     │
│ - Linear(64, 4) → mean              │
│ - learnable log_std                 │
│ - Tanh squashing + 缩放到动作范围   │
└────────────────────┬────────────────┘
                     │
                     ▼
输出: action (4维), log_prob
```

### 2.2 Critic网络（价值网络）

```
输入: observation (10维)
    │
    ▼
┌─────────────────────────────────────┐
│ LayerNorm (特征归一化)              │
└────────────────────┬────────────────┘
                     │
                     ▼
┌─────────────────────────────────────┐
│ Linear(10, 64) + ReLU + LayerNorm   │
└────────────────────┬────────────────┘
                     │
                     ▼
┌─────────────────────────────────────┐
│ Linear(64, 64) + ReLU + LayerNorm   │  × layer_N次
└────────────────────┬────────────────┘
                     │
                     ▼
┌─────────────────────────────────────┐
│ Linear(64, 1)                       │
└────────────────────┬────────────────┘
                     │
                     ▼
输出: value (1维)
```

### 2.3 有界动作层（BoundedACTLayer）

**核心机制**：使用 **Tanh-Normal 分布** 实现有界连续动作

```python
class TanhNormal:
    def sample(self):
        z = Normal(loc, scale).rsample()  # 从正态分布采样
        return torch.tanh(z)              # Tanh压缩到[-1, 1]
    
    def to_action_space(self, x):
        # 从[-1, 1]映射到实际动作范围
        return x * action_scale + action_bias
    
    def log_probs(self, actions):
        # 计算log概率，需要考虑Tanh变换的雅可比行列式
        tanh_actions = from_action_space(actions)  # 映射回[-1, 1]
        z = inverse_tanh(tanh_actions)
        log_prob = Normal.log_prob(z)
        log_prob -= log(1 - tanh_actions²)  # 雅可比修正
        return log_prob
```

---

## 三、训练流程

### 3.1 整体训练流程图

```
┌─────────────────────────────────────────────────────────────────┐
│                      PPO 训练流程                               │
└─────────────────────────────────────────────────────────────────┘

初始化: 环境、网络、Buffer、优化器
    │
    ▼
┌─────────────────────────────────────────────────────────────────┐
│                     外层循环: Episodes                          │
│  ┌───────────────────────────────────────────────────────────┐  │
│  │                   内层循环: Steps                          │  │
│  │                                                           │  │
│  │  1. 收集动作                                              │  │
│  │     policy.get_actions(obs, rnn_states, ...)              │  │
│  │     → values, actions, log_probs                          │  │
│  │                                                           │  │
│  │  2. 环境交互                                              │  │
│  │     env.step(actions)                                     │  │
│  │     → obs, reward, done, info                             │  │
│  │                                                           │  │
│  │  3. 存储经验                                              │  │
│  │     buffer.insert(obs, actions, rewards, ...)             │  │
│  │                                                           │  │
│  │  4. step++                                                │  │
│  │                                                           │  │
│  │  直到: step == episode_length                             │  │
│  └───────────────────────────────────────────────────────────┘  │
│                                                                 │
│  5. 计算回报 (GAE)                                              │
│     buffer.compute_returns(next_value)                          │
│                                                                 │
│  6. PPO更新                                                     │
│     ┌───────────────────────────────────────────────────────┐   │
│  │     for epoch in ppo_epoch:                              │   │
│  │         for mini_batch in buffer:                        │   │
│  │             # 评估动作                                    │   │
│  │             values, log_probs, entropy =                 │   │
│  │                 policy.evaluate_actions(...)             │   │
│  │                                                           │   │
│  │             # 计算重要性比率                              │   │
│  │             ratio = exp(log_prob - old_log_prob)         │   │
│  │                                                           │   │
│  │             # PPO裁剪目标                                 │   │
│  │             surr1 = ratio × advantage                     │   │
│  │             surr2 = clip(ratio, 1-ε, 1+ε) × advantage    │   │
│  │             policy_loss = -min(surr1, surr2)             │   │
│  │                                                           │   │
│  │             # 价值损失                                    │   │
│  │             value_loss = (value - return)²               │   │
│  │                                                           │   │
│  │             # 总损失                                      │   │
│  │             loss = value_loss + policy_loss - entropy    │   │
│  │                                                           │   │
│  │             # 梯度更新                                    │   │
│  │             optimizer.zero_grad()                        │   │
│  │             loss.backward()                              │   │
│  │             clip_grad_norm_(params, max_grad_norm)       │   │
│  │             optimizer.step()                             │   │
│  └───────────────────────────────────────────────────────────┘  │
│                                                                 │
│  7. 清空Buffer                                                  │
│                                                                 │
│  直到: total_steps >= num_env_steps                             │
└─────────────────────────────────────────────────────────────────┘
```

### 3.2 GAE（Generalized Advantage Estimation）

**优势函数计算**：
```
δ_t = r_t + γ × V(s_{t+1}) × mask - V(s_t)
A_t = δ_t + (γλ) × mask × A_{t+1}

其中：
- γ: 折扣因子（默认0.99）
- λ: GAE参数（默认0.95）
- mask: 1 - done（episode结束时为0）
```

**回报计算**：
```
return_t = A_t + V(s_t)
```

---

## 四、经验回放机制

### 4.1 Buffer数据结构

```python
class PPOReplayBuffer:
    # 数据形状: (episode_length + 1, n_rollout_threads, ...)
    
    obs: np.ndarray           # 观测
    actions: np.ndarray       # 动作
    rewards: np.ndarray       # 奖励
    value_preds: np.ndarray   # 价值预测
    returns: np.ndarray       # 回报
    action_log_probs: np.ndarray  # 动作log概率
    masks: np.ndarray         # 掩码（1 - done）
    rnn_states: np.ndarray    # RNN隐藏状态
    rnn_states_critic: np.ndarray  # Critic RNN状态
```

### 4.2 数据流

```
收集阶段:
┌─────────────────────────────────────────────────────────────┐
│  step=0   step=1   step=2   ...   step=T-1   step=T       │
│  ────────────────────────────────────────────────────────  │
│  obs[0]   obs[1]   obs[2]   ...   obs[T-1]   obs[T]       │
│           act[0]   act[1]   ...   act[T-1]                 │
│           rew[0]   rew[1]   ...   rew[T-1]                 │
│           val[0]   val[1]   ...   val[T-1]                 │
│           log[0]   log[1]   ...   log[T-1]                 │
│           msk[1]   msk[2]   ...   msk[T]                   │
└─────────────────────────────────────────────────────────────┘

训练阶段:
┌─────────────────────────────────────────────────────────────┐
│  展平为: (episode_length × n_rollout_threads, ...)         │
│  随机打乱后生成 mini_batch                                   │
└─────────────────────────────────────────────────────────────┘
```

### 4.3 Mini-batch生成

```python
def feed_forward_generator(self, advantages, num_mini_batch):
    # 1. 计算batch大小
    batch_size = episode_length × n_rollout_threads
    
    # 2. 随机打乱索引
    indices = torch.randperm(batch_size)
    
    # 3. 划分mini_batch
    for i in range(num_mini_batch):
        batch_indices = indices[i * mini_batch_size : (i+1) * mini_batch_size]
        yield (
            obs[batch_indices],
            actions[batch_indices],
            ...  # 其他数据
        )
```

---

## 五、与环境交互接口

### 5.1 交互流程

```python
# 1. 初始化
env = SheepFlockEnv(...)
policy = PPOActorCritic(...)
buffer = PPOReplayBuffer(...)

# 2. 重置环境
obs = env.reset()
buffer.obs[0] = obs

# 3. 收集经验
for step in range(episode_length):
    # 获取动作
    values, actions, log_probs, rnn_states, rnn_states_critic = \
        policy.get_actions(
            buffer.obs[step],
            buffer.rnn_states[step],
            buffer.rnn_states_critic[step],
            buffer.masks[step],
        )
    
    # 环境交互
    obs, reward, done, info = env.step(actions)
    
    # 存储经验
    buffer.insert(obs, rnn_states, rnn_states_critic, 
                  actions, log_probs, values, rewards, masks)

# 4. 计算回报
next_value = policy.get_values(buffer.obs[-1], ...)
buffer.compute_returns(next_value)

# 5. 训练更新
for epoch in range(ppo_epoch):
    for batch in buffer.feed_forward_generator(advantages):
        # PPO更新...
```

### 5.2 关键接口说明

| 方法 | 输入 | 输出 | 说明 |
|------|------|------|------|
| `env.reset()` | - | obs | 重置环境，返回初始观测 |
| `env.step(actions)` | actions (4,) | obs, reward, done, info | 执行动作，返回结果 |
| `policy.get_actions()` | obs, rnn_states, masks | values, actions, log_probs, rnn_states | 采样动作 |
| `policy.get_values()` | obs, rnn_states, masks | values | 获取价值估计 |
| `policy.evaluate_actions()` | obs, actions, ... | values, log_probs, entropy | 评估动作（训练用） |
| `buffer.insert()` | 经验数据 | - | 存储经验 |
| `buffer.compute_returns()` | next_value | - | 计算GAE回报 |

---

## 六、训练一个Episode的完整数据流

```
┌─────────────────────────────────────────────────────────────────┐
│                    Episode 数据流                                │
└─────────────────────────────────────────────────────────────────┘

Step 0:
  env.reset() → obs_0
  policy.get_actions(obs_0) → value_0, action_0, log_prob_0
  env.step(action_0) → obs_1, reward_0, done_0
  buffer.insert(obs_1, action_0, reward_0, value_0, log_prob_0, ...)

Step 1:
  policy.get_actions(obs_1) → value_1, action_1, log_prob_1
  env.step(action_1) → obs_2, reward_1, done_1
  buffer.insert(obs_2, action_1, reward_1, value_1, log_prob_1, ...)

...

Step T-1:
  policy.get_actions(obs_{T-1}) → value_{T-1}, action_{T-1}, log_prob_{T-1}
  env.step(action_{T-1}) → obs_T, reward_{T-1}, done_{T-1}
  buffer.insert(obs_T, action_{T-1}, reward_{T-1}, value_{T-1}, log_prob_{T-1}, ...)

Episode结束:
  policy.get_values(obs_T) → next_value
  buffer.compute_returns(next_value)
  
  # GAE计算
  for t = T-1 to 0:
    δ_t = reward_t + γ × value_{t+1} × mask - value_t
    A_t = δ_t + γλ × mask × A_{t+1}
    return_t = A_t + value_t

  # PPO更新
  for epoch in ppo_epoch:
    for mini_batch in buffer:
      values, log_probs, entropy = policy.evaluate_actions(...)
      ratio = exp(log_prob - old_log_prob)
      loss = compute_ppo_loss(ratio, advantages, values, returns, entropy)
      optimizer.step()
```

---

## 七、关键类/函数职责

### 7.1 PPOTrainer (train_ppo.py)

| 方法 | 职责 |
|------|------|
| `__init__` | 初始化环境、网络、Buffer、优化器 |
| `warmup` | 预热，收集初始观测 |
| `collect` | 从策略采样动作 |
| `insert` | 将经验存入Buffer |
| `compute_returns` | 计算GAE回报 |
| `train` | 执行PPO更新 |
| `save/load` | 保存/加载模型 |
| `run` | 主训练循环 |

### 7.2 PPOActorCritic (ppo_actor_critic.py)

| 方法 | 职责 |
|------|------|
| `get_actions` | 采样动作（推理用） |
| `get_values` | 获取价值估计 |
| `evaluate_actions` | 评估动作（训练用） |

### 7.3 PPOReplayBuffer (ppo_buffer.py)

| 方法 | 职责 |
|------|------|
| `insert` | 存储一步经验 |
| `compute_returns` | 计算GAE回报 |
| `feed_forward_generator` | 生成训练mini-batch |
| `after_update` | 更新后处理 |

### 7.4 BoundedACTLayer (bounded_act.py)

| 方法 | 职责 |
|------|------|
| `forward` | 采样有界动作 |
| `evaluate_actions` | 计算动作log概率和熵 |
