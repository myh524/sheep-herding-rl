# 高层决策控制器项目计划手册

> **项目名称**: High-Layer Decision Controller for Multi-Robot Sheep Herding
> **版本**: v1.0
> **最后更新**: 2026-02-15
> **技术总监**: AI Assistant

---

## 当前进度速览

**阶段**: 阶段3 - 训练调试
**进度**: 刚开始

### 已完成
- [x] 创建项目目录结构
- [x] 编写项目计划文档
- [x] E-02: 实现SheepEntity实体类
- [x] E-03: 实现SheepScenario场景类
- [x] E-04: 实现SheepFlockEnv环境类
- [x] E-05: 设计观测空间
- [x] E-06: 设计动作空间
- [x] E-07: 实现奖励函数
- [x] E-08: 环境单元测试
- [x] N-01: 修改Actor网络输出层
- [x] N-02: 修改Critic网络输入层
- [x] N-03: 实现站位采样模块
- [x] N-04: 网络结构测试

### 进行中
- [ ] T-01: 创建训练入口脚本
- [ ] T-02: 配置训练参数
- [ ] T-03: 实现课程学习
- [ ] T-04: 奖励函数调试

### 下一步
创建训练入口脚本和配置文件

---

## 目录

1. [项目概述](#一项目概述)
2. [系统架构](#二系统架构)
3. [技术栈](#三技术栈)
4. [开发阶段规划](#四开发阶段规划)
5. [模块详细设计](#五模块详细设计)
6. [接口规范](#六接口规范)
7. [测试计划](#七测试计划)
8. [风险管理](#八风险管理)
9. [里程碑与交付物](#九里程碑与交付物)
10. [附录](#十附录)

---

## 一、项目概述

### 1.1 项目背景

本项目旨在开发一个**基于MAPPO算法的高层决策控制器**，用于多机器人羊群引导任务。控制器通过分析羊群状态信息，输出机械狗的最优站位参数，实现高效的羊群引导。

### 1.2 项目目标

| 目标类型 | 具体目标 | 衡量指标 |
|---------|---------|---------|
| 功能目标 | 实现羊群状态感知 | 质心、形状、扩散度准确率 > 95% |
| 功能目标 | 输出合理站位参数 | 羊群引导成功率 > 80% |
| 性能目标 | 实时决策响应 | 推理延迟 < 100ms |
| 性能目标 | 多智能体协同 | 协作效率提升 > 30% |

### 1.3 核心设计理念

分层控制架构：
- **高层控制器 (本项目)**: 羊群状态感知、策略决策、站位规划
- **低层控制器 (外部系统)**: 运动控制、轨迹跟踪、避障控制

---

## 二、系统架构

### 2.1 整体架构

系统分为五层：
1. **环境层**: 环境定义、状态管理
2. **执行层**: 训练流程控制
3. **数据层**: 经验存储、GAE计算
4. **策略层**: Actor-Critic网络
5. **算法层**: MAPPO训练器

### 2.2 目录结构规划



---

## 三、技术栈

### 3.1 核心依赖

| 组件 | 版本 | 用途 |
|------|------|------|
| Python | 3.8+ | 主要开发语言 |
| PyTorch | 1.8.1 | 深度学习框架 |
| NumPy | 1.19.4 | 数值计算 |
| Gym | 0.10.5 | 环境接口 |
| TensorBoardX | 2.1 | 训练可视化 |
| Wandb | 0.10.31 | 实验跟踪 |

---

## 四、开发阶段规划

### 4.1 阶段总览

| 阶段 | 名称 | 周期 | 主要任务 |
|------|------|------|---------|
| 阶段1 | 环境开发 | 1-2周 | 环境定义、观测/动作空间、奖励函数 |
| 阶段2 | 网络适配 | 1周 | 网络结构修改、站位采样模块 |
| 阶段3 | 训练调试 | 2-3周 | 训练脚本、课程学习、超参数搜索 |
| 阶段4 | 系统集成 | 1周 | 高层-低层接口、端到端测试 |

### 4.2 阶段1: 环境开发 (1-2周)

#### 任务清单

| 任务ID | 任务描述 | 优先级 | 预计工时 |
|--------|---------|--------|---------|
| E-01 | 创建环境目录结构 | P0 | 0.5天 |
| E-02 | 实现SheepEntity实体类 | P0 | 1天 |
| E-03 | 实现SheepScenario场景类 | P0 | 1天 |
| E-04 | 实现SheepFlockEnv环境类 | P0 | 2天 |
| E-05 | 设计观测空间 | P0 | 1天 |
| E-06 | 设计动作空间 | P0 | 1天 |
| E-07 | 实现奖励函数 | P0 | 1天 |
| E-08 | 环境单元测试 | P1 | 1天 |

#### 观测空间设计

观测向量结构:
- [0:2] 羊群质心位置
- [2:4] 羊群主方向向量
- [4] 羊群扩散度
- [5] 羊群数量
- [6:8] 目标位置 (target_x, target_y)
- [8:10] 当前机械狗位置

#### 动作空间设计

动作向量结构 (连续值):
- [0] 站位半径参数 (radius_mean)
- [1] 半径标准差 (radius_std)
- [2] 聚集角度 (angle_mean)
- [3] 聚集度

### 4.3 阶段2: 网络适配 (1周)

| 任务ID | 任务描述 | 优先级 | 预计工时 |
|--------|---------|--------|---------|
| N-01 | 修改Actor网络输出层 | P0 | 1天 |
| N-02 | 修改Critic网络输入层 | P0 | 1天 |
| N-03 | 实现站位采样模块 | P0 | 1天 |
| N-04 | 网络结构测试 | P1 | 1天 |

### 4.4 阶段3: 训练调试 (2-3周)

| 任务ID | 任务描述 | 优先级 | 预计工时 |
|--------|---------|--------|---------|
| T-01 | 创建训练入口脚本 | P0 | 0.5天 |
| T-02 | 配置训练参数 | P0 | 0.5天 |
| T-03 | 实现课程学习 | P1 | 2天 |
| T-04 | 奖励函数调试 | P0 | 3天 |
| T-05 | 超参数搜索 | P1 | 3天 |

#### 课程学习策略

| 阶段 | 羊群数量 | 机械狗数量 | 目标距离 | 障碍物 |
|------|---------|-----------|---------|--------|
| 阶段1 | 5 | 2 | 近 | 无 |
| 阶段2 | 10 | 3 | 中 | 少量 |
| 阶段3 | 20 | 4 | 远 | 多个 |
| 阶段4 | 30+ | 4-6 | 随机 | 随机 |

### 4.5 阶段4: 系统集成 (1周)

| 任务ID | 任务描述 | 优先级 | 预计工时 |
|--------|---------|--------|---------|
| I-01 | 定义高层-低层接口 | P0 | 1天 |
| I-02 | 实现站位采样转换 | P0 | 1天 |
| I-03 | 端到端测试 | P0 | 2天 |
| I-04 | 文档完善 | P1 | 1天 |

---

## 五、模块详细设计

### 5.1 环境模块

#### SheepFlockEnv 类

- : 初始化环境参数
- : 重置环境，返回初始观测
- : 执行动作，返回
- : 计算当前观测
- : 计算奖励

#### 羊群运动模型

规则:
1. 分离: 避免碰撞
2. 对齐: 方向一致
3. 聚合: 向中心移动
4. 逃避: 远离机械狗

### 5.2 算法模块

#### MAPPO训练器

训练步骤:
1. 计算GAE优势函数
2. 生成mini-batch
3. PPO更新

---

## 六、接口规范

### 6.1 高层-低层接口

FormationParams (站位参数):
- radius_mean: 理想站位半径
- radius_std: 半径标准差
- angle_mean: 聚集角度均值
- concentration: 聚集度 [0, 1]

TargetPosition (目标位置):
- x, y: 坐标
- priority: 优先级

### 6.2 环境接口

- observation_space: 观测空间
- action_space: 动作空间
- share_observation_space: 共享观测空间
- reset(): 重置
- step(actions): 执行动作

---

## 七、测试计划

### 7.1 单元测试

| 测试模块 | 测试内容 | 覆盖率目标 |
|---------|---------|-----------|
| test_env.py | 环境重置、步进、观测计算 | > 90% |
| test_policy.py | 策略网络前向传播 | > 85% |
| test_buffer.py | 数据存储、GAE计算 | > 90% |
| test_mappo.py | PPO更新、损失计算 | > 80% |

### 7.2 性能测试

| 指标 | 目标值 | 测试方法 |
|------|--------|---------|
| 推理延迟 | < 100ms | 时间测量 |
| 内存占用 | < 4GB | 内存监控 |
| 训练速度 | > 1000 steps/s | 吞吐量测试 |
| 收敛步数 | < 5M steps | 学习曲线分析 |

---

## 八、风险管理

### 8.1 风险识别

| 风险ID | 风险描述 | 可能性 | 影响 | 等级 |
|--------|---------|--------|------|------|
| R-01 | 奖励函数设计不当 | 高 | 高 | 高 |
| R-02 | 羊群模型过于简化 | 中 | 高 | 中 |
| R-03 | 训练时间过长 | 中 | 中 | 中 |
| R-04 | 接口不匹配 | 低 | 高 | 低 |
| R-05 | 多智能体协同困难 | 中 | 中 | 中 |

### 8.2 风险应对

| 风险ID | 应对策略 |
|--------|---------|
| R-01 | 设计多套奖励方案，消融实验 |
| R-02 | 逐步增加模型复杂度 |
| R-03 | 使用课程学习，并行训练 |
| R-04 | 提前定义接口规范 |
| R-05 | 从少智能体开始 |

---

## 九、里程碑与交付物

### 9.1 里程碑计划

| 里程碑 | 时间 | 交付物 | 验收标准 |
|--------|------|--------|---------|
| M1 | 第2周 | envs/模块 | 环境可运行 |
| M2 | 第3周 | 网络代码 | 前向传播正确 |
| M3 | 第5周 | 训练脚本 | 训练可收敛 |
| M4 | 第6周 | 完整系统 | 端到端测试通过 |

### 9.2 交付物清单

| 阶段 | 交付物 | 格式 |
|------|--------|------|
| 阶段1 | 环境代码 | Python |
| 阶段1 | 环境测试报告 | Markdown |
| 阶段2 | 网络代码 | Python |
| 阶段3 | 训练脚本 | Python/Shell |
| 阶段4 | 完整系统 | Python |
| 阶段4 | API文档 | Markdown |
| 阶段4 | 用户手册 | Markdown |

---

## 十、附录

### 10.1 术语表

| 术语 | 英文 | 解释 |
|------|------|------|
| MAPPO | Multi-Agent PPO | 多智能体近端策略优化 |
| CTDE | Centralized Training Decentralized Execution | 中心化训练去中心化执行 |
| GAE | Generalized Advantage Estimation | 广义优势估计 |
| 站位参数 | Formation Parameters | 描述机械狗站位分布的参数 |

### 10.2 参考资料

1. MAPPO论文: https://arxiv.org/abs/2103.01955
2. PPO论文: https://arxiv.org/abs/1707.06347
3. OpenAI Gym文档: https://gym.openai.com/docs/
4. PyTorch官方文档: https://pytorch.org/docs/

---

**文档版本历史**

| 版本 | 日期 | 作者 | 变更说明 |
|------|------|------|---------|
| v1.0 | 2026-02-15 | AI Assistant | 初始版本 |

---

*本文档由技术总监编写，如有疑问请联系项目负责人。*