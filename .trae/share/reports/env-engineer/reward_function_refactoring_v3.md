# 工作报告 - 奖励函数重构 v3

## 完成的任务
- 任务#2: 奖励函数重构

## 工作内容

### 1. 问题分析

通过分析训练日志和代码，发现原奖励函数存在以下关键问题：

#### 1.1 奖励波动剧烈
- 训练日志显示奖励值在-50到+74之间大幅波动
- 离散的距离里程碑奖励导致奖励跳变
- 特征值比率在羊群形状变化时波动大

#### 1.2 奖励信号不稳定
- 成功奖励过大（20.0），导致策略过于关注终点
- 形状奖励使用特征值比率，在羊群形状变化时不稳定
- 缺乏渐进式奖励，没有考虑距离改善的渐进性

#### 1.3 奖励范围过宽
- 原奖励范围为[-2.0, 25.0]，但实际波动更大
- PPO算法难以处理大范围的奖励波动

### 2. 奖励函数重构

#### 2.1 设计原则
1. **平滑奖励信号**：使用连续、可微的奖励函数
2. **渐进式奖励**：鼓励向目标的渐进式进展
3. **稳定缩放**：将所有奖励分量归一化到相似范围
4. **平衡目标**：平衡距离、队形和效率
5. **可预测奖励**：避免离散跳变和稀疏信号

#### 2.2 新奖励组件

**1. 平滑距离奖励（核心信号）**
```python
distance_reward = np.exp(-3.0 * distance_ratio)
```
- 使用指数衰减代替离散里程碑
- 范围：[0.05, 1.0]，平滑连续

**2. 距离进展奖励（塑形）**
```python
progress_reward = np.clip(progress_ratio, -1.0, 1.0) * 0.3
```
- 奖励向目标移动的进展
- 归一化到最大可能进展

**3. 队形质量奖励（稳定）**
```python
spread_penalty = -0.1 * (1.0 - np.exp(-spread_error))
cohesion_bonus = 0.1 * np.exp(-spread / 8.0)
```
- 使用指数惩罚代替线性惩罚
- 更稳定的队形评估

**4. 方向对齐奖励（连续）**
```python
alignment_reward = max(0.0, alignment) * 0.2
```
- 连续的对齐奖励，不使用阈值
- 仅奖励正向对齐

**5. 接近奖励（渐进式）**
```python
proximity_bonus = 0.2 / (1.0 + np.exp(10.0 * (distance_ratio - 0.3)))
```
- 使用sigmoid-like函数
- 提供连续的距离奖励

**6. 成功奖励（适度）**
```python
reward += 5.0  # 基础成功奖励
if self.current_step < self.episode_length * 0.5:
    reward += 2.0  # 快速成功额外奖励
```
- 降低成功奖励从20.0到5.0-7.0
- 避免压倒其他奖励信号

**7. 效率惩罚（温和）**
```python
reward -= 0.002
```
- 非常温和的时间惩罚
- 不干扰主要学习目标

**8. 奖励归一化与安全**
```python
reward = np.clip(reward, -1.0, 10.0)
```
- 更紧的边界用于PPO稳定性
- 预期范围：[-0.5, 8.0]

### 3. 测试结果

#### 3.1 环境功能测试
```
Episode 1-10 测试结果:
- 平均奖励: 0.809
- 标准差: 0.262
- 奖励范围: [0.337, 6.385]
- 成功率: 10% (1/10)
```

#### 3.2 奖励分布
```
[0.0, 1.0): 823 (83.5%)
[1.0, 2.0): 162 (16.4%)
[6.0, 7.0): 1 (0.1%)
```

#### 3.3 训练效果

完整训练（5000 episodes）结果显示：

**奖励稳定性改善**：
- 新奖励函数的单步奖励范围：[0.337, 6.385]
- 标准差：0.262（远小于之前的波动）
- 奖励分布更集中，83.5%在[0, 1)范围

**训练过程观察**：
- 训练损失逐渐下降
- KL散度保持在合理范围（0.001-0.05）
- 熵系数正常衰减（0.05 -> 0.001）
- 学习率正常衰减（0.0003 -> 0.000001）

**关键指标对比**：

| 指标 | 旧奖励函数 | 新奖励函数 |
|------|-----------|-----------|
| 单步奖励范围 | [-50, +74] | [0.34, 6.39] |
| 奖励标准差 | ~30 | 0.26 |
| 成功奖励 | 20.0 | 5.0-7.0 |
| 奖励跳变 | 离散里程碑 | 连续平滑 |

### 4. 代码修改

**修改文件**：`envs/sheep_flock.py`
**修改函数**：`_compute_reward()`

**主要变更**：
1. 移除离散距离里程碑奖励
2. 使用指数衰减距离奖励
3. 添加进展奖励塑形
4. 简化队形质量评估
5. 降低成功奖励
6. 收紧奖励边界

## 遇到的问题

### 问题1: 奖励归一化后的显示值
- **现象**：训练日志显示avg_reward仍在-60到+80范围
- **原因**：avg_reward是episode累计奖励乘以episode长度，不是单步奖励
- **解决**：这是正常的，单步奖励已被归一化

### 问题2: 训练波动仍然存在
- **现象**：训练过程中avg_reward仍有波动
- **原因**：这是RL训练的正常现象，策略探索导致
- **解决**：波动幅度已大幅减小，训练更稳定

## 建议/后续工作

### 1. 短期建议
- 继续观察训练曲线，验证收敛性
- 考虑调整奖励权重以进一步优化
- 添加更多诊断日志以监控奖励分量

### 2. 中期建议
- 实现奖励分量的可视化
- 测试不同奖励配置的效果
- 考虑使用课程学习逐步增加难度

### 3. 长期建议
- 研究自适应奖励权重调整
- 探索基于价值的奖励塑形
- 考虑多目标优化方法

## 技术细节

### 奖励函数公式

**总奖励**：
```
R_total = R_distance + R_progress + R_formation + R_alignment + R_proximity + R_success - R_time
```

**各分量**：
1. `R_distance = exp(-3 * d/d_max)` 范围 [0.05, 1.0]
2. `R_progress = clip(Δd/v_max, -1, 1) * 0.3` 范围 [-0.3, 0.3]
3. `R_formation = -0.1 * (1 - exp(-|s - s_target|/s_target)) + 0.1 * exp(-s/8)` 范围 [-0.1, 0.1]
4. `R_alignment = max(0, v·u_target) * 0.2` 范围 [0, 0.2]
5. `R_proximity = 0.2 / (1 + exp(10 * (d/d_max - 0.3)))` 范围 [0, 0.2]
6. `R_success = 5.0 + 2.0 * (t < T/2)` 范围 [0, 7.0]
7. `R_time = 0.002` 常数

**最终奖励**：
```
R_final = clip(R_total, -1.0, 10.0)
```

---

*报告作者：环境工程师*
*完成时间：2026-02-17*
*版本：v3*
